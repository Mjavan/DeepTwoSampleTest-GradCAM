{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15be579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b9c5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Codes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20fda9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runing vis2sampleTest3\n",
    "\n",
    "%run Visualise_Two_Sample_Test3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cc3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3e88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(10, False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3f1858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "encoder\n",
      "encoder.conv1\n",
      "encoder.conv2\n",
      "encoder.conv3\n",
      "encoder.conv4\n",
      "encoder.fc1\n",
      "encoder.fc2_mu\n",
      "encoder.fc2_logvar\n",
      "decoder\n",
      "decoder.fc\n",
      "decoder.deconv1\n",
      "decoder.deconv2\n",
      "decoder.deconv3\n",
      "decoder.deconv4\n"
     ]
    }
   ],
   "source": [
    "for module in vae.named_modules():\n",
    "    print(f'{module[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93e4495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (fc2_mu): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (fc2_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc): Linear(in_features=10, out_features=4096, bias=True)\n",
      "    (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (deconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f65f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a7ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d50866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (fc2_mu): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (fc2_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=10, out_features=4096, bias=True)\n",
       "    (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2175879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "encoder\n",
      "encoder.conv1\n",
      "encoder.conv2\n",
      "encoder.conv3\n",
      "encoder.conv4\n",
      "encoder.fc1\n",
      "encoder.fc2_mu\n",
      "encoder.fc2_logvar\n",
      "decoder\n",
      "decoder.fc\n",
      "decoder.deconv1\n",
      "decoder.deconv2\n",
      "decoder.deconv3\n",
      "decoder.deconv4\n"
     ]
    }
   ],
   "source": [
    "for module in vae.named_modules():\n",
    "    \n",
    "    print(module[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc5f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', VAE(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (fc2_mu): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (fc2_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc): Linear(in_features=10, out_features=4096, bias=True)\n",
      "    (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (deconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "('encoder', Encoder(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (fc2_mu): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (fc2_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
      "))\n",
      "('encoder.conv1', Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('encoder.conv2', Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('encoder.conv3', Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('encoder.conv4', Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('encoder.fc1', Linear(in_features=4096, out_features=512, bias=True))\n",
      "('encoder.fc2_mu', Linear(in_features=512, out_features=10, bias=True))\n",
      "('encoder.fc2_logvar', Linear(in_features=512, out_features=10, bias=True))\n",
      "('decoder', Decoder(\n",
      "  (fc): Linear(in_features=10, out_features=4096, bias=True)\n",
      "  (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "))\n",
      "('decoder.fc', Linear(in_features=10, out_features=4096, bias=True))\n",
      "('decoder.deconv1', ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('decoder.deconv2', ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('decoder.deconv3', ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n",
      "('decoder.deconv4', ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n"
     ]
    }
   ],
   "source": [
    "for module in vae.named_modules():\n",
    "    \n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c356df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder.conv2', Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n"
     ]
    }
   ],
   "source": [
    "for module in vae.named_modules():\n",
    "    \n",
    "    if module[0] == 'encoder.conv2':\n",
    "        \n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f054b",
   "metadata": {},
   "source": [
    "- The layer that we want to backprobagate the gradients on it. \n",
    "\n",
    "- One convelution to the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a215c3",
   "metadata": {},
   "source": [
    "- Note that when we want to backprobagate gradients, we can do it for every of the following layers: 'conv1', 'conv2', 'conv3', 'conv4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ddb61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randn(28,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f4086ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ed666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Visualise_Two_Sample_Test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a1221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721a4c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (fc2_mu): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (fc2_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=10, out_features=4096, bias=True)\n",
       "    (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a807bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder.conv2', Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)))\n"
     ]
    }
   ],
   "source": [
    "for module in vae.named_modules():\n",
    "    \n",
    "    if module[0] == 'encoder.conv2':\n",
    "        \n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3598f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae2 = VAE(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c23ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (fc2_mu): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (fc2_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=10, out_features=4096, bias=True)\n",
       "    (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (deconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10976a36",
   "metadata": {},
   "source": [
    "## Visulaizing Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfb86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd21b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from utils import save_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3cf587",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d66940",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_split = root_dir/'Explainability'/'Reproducibility'/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79cb362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "latentvar ='shape'\n",
    "\n",
    "latentcls= str([1,2])\n",
    "exp=4\n",
    "\n",
    "norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9390400",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(svd_split/f'test_set_dSprites_{seed}_{latentvar}_{latentcls}_{norm}.pickle',\n",
    "        'rb') as f:\n",
    "        testset = pickle.load(f)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e72b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c7f6262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e2d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_dir = root_dir/'Explainability'/'Codes'/'heatmaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bccaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcam_map = np.load(os.path.join(heatmap_dir,\n",
    "                          f'{exp}_{seed}_{latentvar}_{latentcls}_htm.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efea7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_heatmap(x,gcam_map,exp=4):\n",
    "    test_index = 0\n",
    "    x = x.repeat(1, 3, 1, 1)\n",
    "    for i in range(x.size(0)):\n",
    "        if i % 12 == 0:    \n",
    "            raw_image = x[i] * 255.0\n",
    "            ndarr = raw_image.permute(1, 2, 0).cpu().byte().numpy()\n",
    "            im = Image.fromarray(ndarr.astype(np.uint8))\n",
    "            \n",
    "            im_path = os.path.join(heatmap_dir,f'{exp}_{latentvar}_{latentcls}')\n",
    "            os.makedirs(im_path, exist_ok=True)\n",
    "            \n",
    "            im.save(os.path.join(im_path,\"{}-origin.png\".format(test_index)))\n",
    "            attmap_path = os.path.join(im_path,\"{}-attmap.png\".format(test_index))\n",
    "            \n",
    "            im_np = np.asarray(im)\n",
    "            \n",
    "            save_cam(im_np, attmap_path, gcam_map[i])\n",
    "            \n",
    "        print(f'\\nheatmaps were saved')\n",
    "        test_index += 12\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d61fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gcam_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize and save attention maps \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvisualize_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mvisualize_heatmap\u001b[0;34m(x, exp)\u001b[0m\n\u001b[1;32m     14\u001b[0m     attmap_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(im_path,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-attmap.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_index))\n\u001b[1;32m     16\u001b[0m     im_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(im)\n\u001b[0;32m---> 18\u001b[0m     save_cam(im_np, attmap_path, \u001b[43mgcam_map\u001b[49m[i])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mheatmaps were saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m test_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gcam_map' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize and save attention maps \n",
    "visualize_heatmap(x,exp=4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def save_cam(image, gcam):\n",
    "    # Normalize the Grad-CAM values\n",
    "    gcam = (gcam - np.min(gcam)) / (np.max(gcam) - np.min(gcam))\n",
    "    \n",
    "    # Resize Grad-CAM to match the image dimensions\n",
    "    h, w, _ = image.shape\n",
    "    gcam_resized = np.array(Image.fromarray(gcam).resize((w, h), Image.BILINEAR))\n",
    "\n",
    "    # Apply a colormap similar to cv2.applyColorMap\n",
    "    gcam_colored = plt.cm.jet(gcam_resized)[:, :, :3] * 255\n",
    "    gcam_colored = gcam_colored.astype(np.uint8)\n",
    "\n",
    "    # Add Grad-CAM on top of the original image\n",
    "    heatmap = gcam_colored.astype(np.float64)\n",
    "    \n",
    "    overlaid_image = (heatmap + image.astype(np.float64)) / 2\n",
    "    overlaid_image = (overlaid_image / np.max(overlaid_image) * 255).astype(np.uint8)\n",
    "    \n",
    "    return overlaid_image\n",
    "\n",
    "def visualize_heatmap_grid(x, gcam_map, heatmap_dir, args, num_images=10):\n",
    "    if x.size(1) == 1:  # Check if the input is grayscale\n",
    "        x = x.repeat(1, 3, 1, 1)  # Convert to 3 channels if necessary\n",
    "    \n",
    "    fig, axs = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        raw_image = (x[i] * 255.0).clamp(0, 255)  # Ensure values are within range\n",
    "        ndarr = raw_image.permute(1, 2, 0).cpu().byte().numpy()\n",
    "        \n",
    "        # Original image\n",
    "        axs[0, i].imshow(ndarr)\n",
    "        axs[0, i].axis('off')\n",
    "        \n",
    "        # Grad-CAM heatmap overlaid on the image\n",
    "        overlaid_image = save_cam(ndarr, gcam_map[i])\n",
    "        axs[1, i].imshow(overlaid_image)\n",
    "        axs[1, i].axis('off')\n",
    "    \n",
    "    # Save the grid of images and heatmaps\n",
    "    grid_path = os.path.join(heatmap_dir, f'{args.exp}_{args.latentvar}_{str(args.latentcls)}')\n",
    "    os.makedirs(grid_path, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(grid_path, \"grid.png\"))\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# visualize_heatmap_grid(x, gcam_map, 'path/to/save/dir', args, num_images=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Clustering.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e2243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "L1 = [torch.randn(3,4), torch.randn(3,4)]\n",
    "\n",
    "c = torch.cat(L1)\n",
    "\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671ef9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa961bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(128,64,64)\n",
    "\n",
    "b = np.random.randn(128,64,64)\n",
    "\n",
    "c = np.vstack([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e67074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 64, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f158535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dccefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(7000,1,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c9b2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10a45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd8a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(14,10)\n",
    "\n",
    "b = a.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb84002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ec6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d48c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_torch = torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39a7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f985226",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a':2,'b':45, 'c':78}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2ddce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes are the same.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example arrays\n",
    "array1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "array2 = np.array([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Use assert to check if the shapes are the same\n",
    "assert array1.shape == array2.shape, f\"Shapes are different: {array1.shape} vs {array2.shape}\"\n",
    "\n",
    "print(\"Shapes are the same.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cb94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5959f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros_like(a, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd458b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3905c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(2,1,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae742d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 4, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63833923",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2cc5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf260dc",
   "metadata": {},
   "source": [
    "## Checking spatial Attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2932356",
   "metadata": {},
   "source": [
    "- We can apply this spatial attention mechanism to the activation maps before computing the **weighted sum** in Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # Average pooling across channels\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # Max pooling across channels\n",
    "        attention = torch.cat([avg_out, max_out], dim=1)  # Concatenate both maps\n",
    "        attention = self.conv(attention)  # Apply convolution\n",
    "        return self.sigmoid(attention)  # Apply sigmoid to get attention scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
